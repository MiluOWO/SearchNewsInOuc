# SearchNewsInOuc
针对信息科学与工程学院网站的爬虫+中文检索。
整个系统流程如下所示：
![](https://i.imgur.com/Lf4TJ9t.png)

##爬虫模块
爬虫部分主要利用requests，以及XPATH对网页部分进行爬取。
这里只针对学院板块爬取。

通过定位以下代码获取页面数，做循环遍历页面，获取文章ID，最后在当前目录下生成几个文件。


```python
	Page = etree.HTML(r.text).xpath('//*[@id="mainText_Last"]/@href')
```

|文件名|描述|
--- |--- | ---|
|urlAndTitle.txt|存储新闻url以及标题名|
|title.txt|标题|
|/article/xxx.txt|所有新闻内容，文件名即新闻标题名|

爬取完毕之后，转到数据处理模块。

注：此部分没有和后续部分有交互，因为在生成的时候，只需要执行一次即可（初始化）

##数据处理模块
本模块主要使用python的一个中文分词库，通过对标题的分词，再进行分类整理。我们就可以建立一个索引表，通过关键词属于哪个索引号，再找到我们需要的文章ID。

![](https://i.imgur.com/mRVBX4V.jpg)

这是一个字典类型，key部分为生成的关键词，而value的数字代表包含关键字文章标题。

此模块是单独做出了一个函数，用于模块调用。 所以我们在交互模块可以直接调用此函数。 该模块中的函数会返回三个值：word_dict,title_dict,url_dict（关键词索引字典，标题索引字典，url索引字典）

##交互模块
本模块利用了FLASK框架搭建的网站，通过表达提交，再反馈检索结果完成中文文本检索功能。
![](https://i.imgur.com/b1BeuNX.jpg)


##有待改进的问题
###可以对内容进行检索
由于内容分词的话，可以处理，但是在数据处理比较庞大。故本次系统没有对此问题进行解决。

###只针对同源网站下的文章进行爬取
在爬取网页的过程中，发现部分网页是做了URL跳转，所以对那些网页没有做数据抓取工作。
